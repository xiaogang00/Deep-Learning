* SGD

  随机梯度下降，是梯度下降的batch版本。每一次的更新只是使用其中的一个batch的参数，而不是整个数据集：
  $$
  x_{t+1} = x_t + \Delta x_t \\ \Delta x_t = - \eta g_t
  $$
  $g_t$在这里指的是t时刻的梯度

* Momentum

  SGD方法的一个缺点是，其更新方向完全依赖于当前的batch，因而其更新十分不稳定。解决这一问题的一个简单的做法便是引入momentum。
  $$
  \Delta x_t = \rho \Delta x_{t-1} + \eta g_t
  $$
  $\rho$ 和$\eta$ 之和并不需要是1

* Nesterov Momentum

  ​
  $$
  \Delta x_t = \rho \Delta x_{t-1} - \eta \Delta f (x_t + \rho \Delta x_{t-1})
  $$

* Adagrad方法是为各个参数分配不同的学习率的算法
  $$
  \Delta x_t = - \frac{\eta}{\sqrt{\sum\limits_{tau=1}^t g_{\tau}^2 + \varepsilon}}g_t
  $$
  其含义是，对于每个参数，随着其更新的总距离增多，其学习速率也随之变慢。

* Adadelta

  我们可以只使用adagrad的分母中的累计项离当前时间点比较近的项:
  $$
  E[g^2 ]_t = \rho E[g^2]_{t-1} + (1-\rho) g_t^2\\
  \Delta x_t = - \frac{\eta}{\sqrt{E[g^2]_t + \varepsilon}}g_t
  $$
  adagrad中分子部分需要人工设置的初始学习率需要将其消失：
  $$
  \Delta x_t = - \frac{\sqrt{E[\Delta x^2]_{t-1}}}{\sqrt{E[g^2]_t + \varepsilon}}g_t
  $$
  ​