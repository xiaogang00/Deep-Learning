* 7.7多任务学习

  主要是通过合并几个任务中的样例来提高泛化的一种方式。也就是说这些参数能够被多个任务所共享，并且以此来将参数推向泛化更好的方向。

  额外的训练样本是用相同的方式将模型的参数推向泛化更好的方向。这是因为共享参数之后，统计强度可以大大提高，能够改善泛化和泛化误差的范围。

* 7.8提前终止（early stopping）

  当训练算法终止的时候，我们返回的参数不是最新的参数，而是当验证集上的误差在事先指定的循环内没有进一步改善的时候，算法就会终止。

  但是有显著的代价，主要是必须要在训练期间定期评估验证集，以及需要保持最佳的参数副本。

* 格点搜索：

  当有超过三个或者更少的超参数的时候，可以使用格点搜索。格点搜索带来的明显问题就是计算代价随着超参数的数量呈指数级上升

* 调试技巧

  我们无法预知算法的预期行为，另外大部分的机器学习模型有多个自适应的部分。可能会将我们算法中的错误进行自适应的补偿。

  如此一来我们就难以发现错误。可以借助可视化等手段来进行改进。

* 条件对数似然和均方误差

  条件最大似然估计是：
  $$
  \theta_{ML} = arg\max \limits_{\theta} P(\textbf{Y}|\textit{X};\theta)
  $$
  最大似然的本质就在于，它被证明是当样本数目$m\rightarrow\infty$时就收敛率而言是最好的渐近估计。

* ​